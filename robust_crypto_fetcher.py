#!/usr/bin/env python3
"""
Robust Cryptocurrency Data Generator for NeuroGen-Alpha Trading System

This script provides multiple data sources and fallback mechanisms:
1. Real API data from free sources
2. Realistic synthetic data generation for testing
3. Historical pattern simulation
4. Technical indicators and neural network features

Author: NeuroGen-Alpha Project
Version: 1.0 (Robust)
"""

import os
import sys
import json
import time
import logging
import argparse
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import random
import math

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RobustCryptoDataFetcher:
    """Robust cryptocurrency data fetcher with multiple fallback options"""
    
    def __init__(self, output_dir: str = "crypto_data_csv"):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Free API endpoints (no key required)
        self.free_apis = [
            {
                'name': 'coinapi_free',
                'base_url': 'https://rest.coinapi.io/v1',
                'headers': {},
                'rate_limit': 2.0
            },
            {
                'name': 'binance_public',
                'base_url': 'https://api.binance.com/api/v3',
                'headers': {},
                'rate_limit': 1.0
            }
        ]
        
        self.symbol_mappings = {
            'BTCUSD': {
                'binance': 'BTCUSDT',
                'coingecko': 'bitcoin'
            },
            'ETHUSD': {
                'binance': 'ETHUSDT',
                'coingecko': 'ethereum'
            },
            'ADAUSD': {
                'binance': 'ADAUSDT',
                'coingecko': 'cardano'
            },
            'SOLUSD': {
                'binance': 'SOLUSDT',
                'coingecko': 'solana'
            }
        }
        
        # Base prices for synthetic data generation
        self.base_prices = {
            'BTCUSD': 45000,
            'ETHUSD': 3000,
            'ADAUSD': 0.8,
            'SOLUSD': 150
        }
    
    def fetch_binance_data(self, symbol: str, interval: str = '1h', limit: int = 168) -> Optional[pd.DataFrame]:
        """Fetch data from Binance public API"""
        try:
            binance_symbol = self.symbol_mappings.get(symbol, {}).get('binance')
            if not binance_symbol:
                logger.warning(f"No Binance mapping for {symbol}")
                return None
            
            url = "https://api.binance.com/api/v3/klines"
            params = {
                'symbol': binance_symbol,
                'interval': interval,
                'limit': limit
            }
            
            logger.info(f"Fetching {limit} {interval} candles from Binance for {symbol}")
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code != 200:
                logger.warning(f"Binance API returned {response.status_code}")
                return None
            
            data = response.json()
            if not data:
                logger.warning("No data returned from Binance")
                return None
            
            # Convert to DataFrame
            df_data = []
            for candle in data:
                df_data.append({
                    'timestamp': pd.to_datetime(int(candle[0]), unit='ms'),
                    'open': float(candle[1]),
                    'high': float(candle[2]),
                    'low': float(candle[3]),
                    'close': float(candle[4]),
                    'volume': float(candle[5])
                })
            
            df = pd.DataFrame(df_data)
            logger.info(f"Successfully fetched {len(df)} records from Binance")
            return df
            
        except Exception as e:
            logger.error(f"Binance fetch failed: {e}")
            return None
    
    def generate_synthetic_data(self, symbol: str, days: int = 7, interval: str = '1h') -> pd.DataFrame:
        """Generate realistic synthetic cryptocurrency data"""
        logger.info(f"Generating synthetic data for {symbol}: {days} days, {interval} interval")
        
        # Calculate number of data points
        if interval == '5m':
            points_per_day = 288  # 24 * 60 / 5
        elif interval == '1h':
            points_per_day = 24
        elif interval == '1d':
            points_per_day = 1
        else:
            points_per_day = 24  # Default to hourly
        
        total_points = days * points_per_day
        
        # Get base price
        base_price = self.base_prices.get(symbol, 1000)
        
        # Generate timestamps
        end_time = datetime.now()
        if interval == '5m':
            freq = '5min'
        elif interval == '1h':
            freq = '1H'
        else:
            freq = '1D'
        
        timestamps = pd.date_range(
            end=end_time,
            periods=total_points,
            freq=freq
        )
        
        # Generate price data with realistic patterns
        df_data = []
        current_price = base_price
        
        for i, ts in enumerate(timestamps):
            # Add some trending behavior
            trend_factor = 1 + 0.0001 * math.sin(i / (total_points / 4))
            
            # Add volatility
            volatility = 0.02  # 2% volatility
            price_change = np.random.normal(0, volatility)
            
            # Apply changes
            current_price *= (trend_factor + price_change)
            
            # Generate OHLC from current price
            noise = np.random.normal(0, 0.005)  # 0.5% noise
            open_price = current_price * (1 + noise)
            close_price = current_price * (1 + np.random.normal(0, 0.005))
            
            high_price = max(open_price, close_price) * (1 + abs(np.random.normal(0, 0.003)))
            low_price = min(open_price, close_price) * (1 - abs(np.random.normal(0, 0.003)))
            
            # Generate volume
            base_volume = 1000000
            volume = base_volume * (1 + np.random.normal(0, 0.3))
            
            df_data.append({
                'timestamp': ts,
                'open': round(open_price, 2),
                'high': round(high_price, 2),
                'low': round(low_price, 2),
                'close': round(close_price, 2),
                'volume': round(volume, 0)
            })
        
        df = pd.DataFrame(df_data)
        logger.info(f"Generated {len(df)} synthetic data points")
        return df
    
    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add comprehensive technical indicators"""
        df = df.copy()
        
        # Basic price features
        df['price_change'] = df['close'].pct_change()
        df['price_change_abs'] = np.abs(df['price_change'])
        df['log_return'] = np.log(df['close'] / df['close'].shift(1))
        
        # Moving averages
        for period in [5, 10, 20]:
            df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
            df[f'price_vs_sma_{period}'] = (df['close'] - df[f'sma_{period}']) / df[f'sma_{period}']
        
        # RSI (simplified calculation)
        def calculate_rsi(prices, window=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / loss
            return 100 - (100 / (1 + rs))
        
        df['rsi_14'] = calculate_rsi(df['close'])
        
        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # Bollinger Bands
        bb_period = 20
        df['bb_middle'] = df['close'].rolling(window=bb_period).mean()
        bb_std = df['close'].rolling(window=bb_period).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Volatility
        df['volatility_5'] = df['price_change'].rolling(window=5).std()
        df['volatility_20'] = df['price_change'].rolling(window=20).std()
        
        # Volume indicators
        df['volume_sma_10'] = df['volume'].rolling(10).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma_10']
        
        # Time-based features
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        
        # Market microstructure
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])
        
        # Neural network features
        for lag in [1, 2, 3, 5]:
            df[f'close_lag_{lag}'] = df['close'].shift(lag)
            df[f'return_lag_{lag}'] = df['price_change'].shift(lag)
        
        # Momentum features
        df['momentum_3'] = df['close'] / df['close'].shift(3) - 1
        df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        
        return df
    
    def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalize features for neural network training"""
        df = df.copy()
        
        # Select numeric columns for normalization
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        exclude_cols = ['timestamp', 'hour', 'day_of_week', 'is_weekend', 'open', 'high', 'low', 'close', 'volume']
        normalize_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        for col in normalize_cols:
            if col in df.columns:
                # Rolling normalization to prevent data leakage
                rolling_mean = df[col].rolling(window=20, min_periods=1).mean()
                rolling_std = df[col].rolling(window=20, min_periods=1).std()
                df[f'{col}_norm'] = (df[col] - rolling_mean) / (rolling_std + 1e-8)
        
        return df
    
    def fetch_with_fallback(self, symbol: str, days: int = 7, interval: str = '1h') -> pd.DataFrame:
        """Fetch data with multiple fallback options"""
        
        # Try Binance first
        df = self.fetch_binance_data(symbol, interval, days * 24)
        if df is not None and len(df) > 0:
            logger.info(f"Successfully fetched real data from Binance for {symbol}")
            return df
        
        # Fallback to synthetic data
        logger.info(f"Falling back to synthetic data generation for {symbol}")
        return self.generate_synthetic_data(symbol, days, interval)
    
    def save_to_csv(self, df: pd.DataFrame, symbol: str, suffix: str = "") -> Optional[str]:
        """Save DataFrame to CSV"""
        if df is None or df.empty:
            logger.error("No data to save")
            return None
        
        try:
            # Generate filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{symbol}_{timestamp}{suffix}.csv"
            filepath = os.path.join(self.output_dir, filename)
            
            # Sort by timestamp
            df_sorted = df.sort_values('timestamp')
            
            # Save to CSV
            df_sorted.to_csv(filepath, index=False)
            
            logger.info(f"Saved {len(df_sorted)} records to {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Failed to save CSV: {e}")
            return None
    
    def fetch_and_save_historical(self, symbol: str, days: int = 7, interval: str = '1h') -> Optional[str]:
        """Fetch historical data and save to CSV"""
        df = self.fetch_with_fallback(symbol, days, interval)
        if df is not None:
            # Add technical indicators
            df = self.add_technical_indicators(df)
            # Normalize features
            df = self.normalize_features(df)
            
            suffix = f"_historical_{days}d_{interval}"
            return self.save_to_csv(df, symbol, suffix)
        return None
    
    def generate_realtime_data(self, symbol: str) -> Optional[str]:
        """Generate current price data"""
        # Get base price and add some randomness
        base_price = self.base_prices.get(symbol, 1000)
        current_price = base_price * (1 + np.random.normal(0, 0.01))  # 1% volatility
        
        now = datetime.now()
        
        # Create single-row DataFrame
        df = pd.DataFrame([{
            'timestamp': now,
            'open': current_price * 0.999,
            'high': current_price * 1.001,
            'low': current_price * 0.998,
            'close': current_price,
            'volume': 1000000 * (1 + np.random.normal(0, 0.2))
        }])
        
        # Add basic indicators
        df['price_change'] = 0.0
        df['rsi_14'] = 50.0
        df['volume_ratio'] = 1.0
        df['hour'] = now.hour
        df['day_of_week'] = now.weekday()
        df['is_weekend'] = int(now.weekday() >= 5)
        
        suffix = "_realtime"
        return self.save_to_csv(df, symbol, suffix)
    
    def start_realtime_monitoring(self, symbols: List[str], update_interval: int = 60):
        """Start real-time monitoring"""
        logger.info(f"Starting real-time monitoring for {symbols}")
        logger.info(f"Update interval: {update_interval} seconds")
        
        try:
            while True:
                for symbol in symbols:
                    try:
                        filepath = self.generate_realtime_data(symbol)
                        if filepath:
                            # Get current price for logging
                            df = pd.read_csv(filepath)
                            current_price = df['close'].iloc[0]
                            logger.info(f"{symbol}: ${current_price:.2f} -> {filepath}")
                        else:
                            logger.warning(f"Failed to generate data for {symbol}")
                    except Exception as e:
                        logger.error(f"Error updating {symbol}: {e}")
                
                logger.info(f"Sleeping for {update_interval} seconds...")
                time.sleep(update_interval)
                
        except KeyboardInterrupt:
            logger.info("Real-time monitoring stopped by user")

def main():
    parser = argparse.ArgumentParser(description='Robust Cryptocurrency Data Fetcher for NeuroGen-Alpha')
    parser.add_argument('--symbol', default='BTCUSD', help='Trading pair symbol (default: BTCUSD)')
    parser.add_argument('--days', type=int, default=7, help='Days of historical data (default: 7)')
    parser.add_argument('--interval', default='1h', choices=['5m', '1h', '1d'], 
                       help='Data interval (default: 1h)')
    parser.add_argument('--realtime', action='store_true', help='Enable real-time monitoring')
    parser.add_argument('--update-interval', type=int, default=60, 
                       help='Real-time update interval in seconds (default: 60)')
    parser.add_argument('--multiple-symbols', nargs='+', 
                       help='Fetch data for multiple symbols')
    parser.add_argument('--synthetic-only', action='store_true',
                       help='Use only synthetic data generation (for testing)')
    
    args = parser.parse_args()
    
    fetcher = RobustCryptoDataFetcher()
    
    # Determine which symbols to process
    symbols = args.multiple_symbols if args.multiple_symbols else [args.symbol]
    
    print("üöÄ NeuroGen-Alpha Robust Cryptocurrency Data Fetcher")
    print("=" * 60)
    print(f"Symbols: {', '.join(symbols)}")
    print(f"Mode: {'Real-time' if args.realtime else 'Historical'}")
    if args.synthetic_only:
        print("‚ö†Ô∏è  SYNTHETIC DATA MODE (for testing)")
    print("=" * 60)
    
    if args.realtime:
        # Real-time monitoring mode
        fetcher.start_realtime_monitoring(symbols, args.update_interval)
    else:
        # Historical data mode
        for symbol in symbols:
            logger.info(f"Processing {symbol}")
            logger.info(f"Period: {args.days} days, Interval: {args.interval}")
            
            if args.synthetic_only:
                # Force synthetic data
                df = fetcher.generate_synthetic_data(symbol, args.days, args.interval)
                df = fetcher.add_technical_indicators(df)
                df = fetcher.normalize_features(df)
                filepath = fetcher.save_to_csv(df, symbol, f"_synthetic_{args.days}d_{args.interval}")
            else:
                filepath = fetcher.fetch_and_save_historical(symbol, args.days, args.interval)
            
            if filepath:
                logger.info(f"‚úÖ Successfully saved data to {filepath}")
                
                # Also generate current price
                realtime_file = fetcher.generate_realtime_data(symbol)
                if realtime_file:
                    logger.info(f"‚úÖ Current price saved to {realtime_file}")
            else:
                logger.error(f"‚ùå Failed to fetch data for {symbol}")
    
    logger.info("Data fetching completed!")
    logger.info(f"CSV files saved in: {fetcher.output_dir}/")
    
    # Show available symbols
    print("\nüìà Available symbols:")
    for symbol in fetcher.base_prices.keys():
        print(f"  - {symbol}")

if __name__ == "__main__":
    main()
